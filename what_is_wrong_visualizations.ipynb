{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is wrong with Continual Learning in Medical Image Segmentation - Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## of AE reconstructions\n",
    "By using these commands you can generate reconstructions images using your trained autoencoders.\n",
    "The images will be stored to .../evaluation/nnUNet_ext/2d/Task008_mHeartA/Task008_mHeartA/<TrainerMethod\\>__nnUNetPlansv2.1/Generic_UNet/SEQ/head_None/fold_0/Preds_Task008_mHeartA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nnUNet_inference 2d nnUNetTrainerExpertGateUNet -f 0 -trained_on 8 -use_model 8 -d 0 -evaluate_on 8\n",
    "!nnUNet_inference 2d nnUNetTrainerExpertGateUNet -f 0 -trained_on 9 -use_model 9 -d 0 -evaluate_on 8\n",
    "\n",
    "!nnUNet_inference 2d nnUNetTrainerExpertGateMonai -f 0 -trained_on 8 -use_model 8 -d 0 -evaluate_on 8\n",
    "!nnUNet_inference 2d nnUNetTrainerExpertGateMonai -f 0 -trained_on 9 -use_model 9 -d 0 -evaluate_on 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize these files by using some medical imaging software (e.g. [MITK](https://www.mitk.org/wiki/The_Medical_Imaging_Interaction_Toolkit_(MITK) )).\n",
    "\n",
    "Since the nnUNet framework preprocesses the images before passing it to the network, getting the original image with the same preprocessing applied as the reconstructions is a bit tricky: \n",
    "The way we did this, is removing the reconstructed images from one of our autoencoders (if inference has been done before).\n",
    "After that, we change the forward method of that autoencoder to be an identity mapping. You can find the classes at .../Lifelong-nnUNet/nnunet_ext/network_architecture. The classes you may want to adapt are:\n",
    "* expert_gate_UNet\n",
    "* expert_gate_autoencoder\n",
    "* ExpertGateMonaiAutoencoder (in expert_gate_monai_ae.py)<br>\n",
    "\n",
    "Note, that you only need to change a single autoencoder. After changing, the forward method could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x):\n",
    "        return x\n",
    "        skips = []\n",
    "        seg_outputs = []\n",
    "        for d in range(len(self.conv_blocks_context) - 1):#down\n",
    "            x = self.conv_blocks_context[d](x)\n",
    "            skips.append(x)\n",
    "            if not self.convolutional_pooling:\n",
    "                x = self.td[d](x)\n",
    "\n",
    "        x = self.conv_blocks_context[-1](x)\n",
    "\n",
    "        for u in range(len(self.tu)):#up\n",
    "            x = self.tu[u](x)\n",
    "            x = torch.cat((x, skips[-(u + 1)]), dim=1)\n",
    "            x = self.conv_blocks_localization[u](x)\n",
    "            seg_outputs.append(self.seg_outputs[u](x))\n",
    "\n",
    "        return seg_outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying inference with that model, the original images will be exported with the same preprocessing applied. **Do not forget to revert the changes after you are done.**\n",
    "## of residual images\n",
    "After exporting your images as well as the original image, you can compute the residual images. Our exported images of the reconstructions using the modesl trained on task 8 and 9 are called _8.png_ , _9.png_ respectily. The original image is called _gt.png_. To get good visual results you may need to adjust the precentage parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "FOLDER = \"Siemens_UNet/\"\n",
    "image1 = \"8.png\"\n",
    "image2 = \"9.png\"\n",
    "percentage = 0.6 #should be in range (0,1]. 1: Use whole image value range.\n",
    "\n",
    "\n",
    "# Load the two images\n",
    "gt = plt.imread(FOLDER + \"gt.png\")\n",
    "img1 = plt.imread(FOLDER + image1)\n",
    "img2 = plt.imread(FOLDER + image2)\n",
    "\n",
    "# Compute the residual image\n",
    "residual_image1 = np.abs(gt - img1)\n",
    "residual_image2 = np.abs(gt - img2)\n",
    "\n",
    "residual_image1 = residual_image1[:,:,0]\n",
    "residual_image2 = residual_image2[:,:,0]\n",
    "\n",
    "maX = max(np.amax(residual_image1), np.amax(residual_image2))\n",
    "miN = min(np.amin(residual_image1), np.amax(residual_image2))\n",
    "maX -= percentage * maX\n",
    "\n",
    "print(np.mean(np.square(residual_image1)))\n",
    "print(np.mean(np.square(residual_image2)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N = 256\n",
    "A = 256 //4\n",
    "vals = np.ones((N, 4))\n",
    "\n",
    "vals[:, 0] = np.concatenate((np.linspace(  0/256,    0/256, A), np.linspace(  0/256,    0/256, A), np.linspace(    0/256,  256/256, A), np.linspace(256/256,   256/256, N - 3*A)))\n",
    "vals[:, 1] = np.concatenate((np.linspace(  0/256,  256/256, A), np.linspace(256/256,  256/256, A), np.linspace(  256/256,  256/256, A), np.linspace(256/256,  0/256, N - 3*A)))\n",
    "vals[:, 2] = np.concatenate((np.linspace(256/256,  256/256, A), np.linspace(256/256,    0/256, A), np.linspace(    0/256,    0/256, A), np.linspace(  0/256,  0/256, N - 3*A)))\n",
    "newcmp = matplotlib.colors.ListedColormap(vals)\n",
    "\n",
    "\n",
    "\n",
    "# Show the residual image\n",
    "plt.imshow(residual_image1, vmin=miN, vmax=maX, cmap=newcmp)\n",
    "plt.colorbar()\n",
    "plt.savefig(FOLDER + \"residual_\" + image1 + \".svg\")\n",
    "\n",
    "plt.imshow(residual_image2, vmin=miN, vmax=maX, cmap=newcmp)\n",
    "plt.savefig(FOLDER + \"residual_\" + image2 + \".svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
