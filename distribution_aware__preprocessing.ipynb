{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution-Aware Replay for Continual MRI Segmentation - Preprocessing\n",
    "In this section we want to cover the preprocessing and preparation of the data used in our paper.\n",
    "This section assumes that you have set the nnUNet related paths according to [this file](documentation/setting_up_paths.md).\n",
    "\n",
    "Please download the hippocampus and prostate data from these links and place them in the `nnUNet_data_base/nnUNet_raw_data` folder.\n",
    "\n",
    "\n",
    "| Anatomy     |Link                            \n",
    "|-------------|-----------------------------------\n",
    "| Hippocampus | http://medicaldecathlon.com/      \n",
    "|             | https://datadryad.org/stash/dataset/doi:10.5061/dryad.gc72v\n",
    "|             | http://www.hippocampal-protocol.net/SOPs/index.php\n",
    "| Prostate    | https://liuquande.github.io/SAML/ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of the Hippocampus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"This script is not yet implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing you should have the following datasets:\n",
    "- Task097_DecathHip\n",
    "- Task098_Dryad\n",
    "- Task099_HarP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of the Prostate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from nnunet.dataset_conversion.utils import generate_dataset_json\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def copyMetaData(original, img):\n",
    "    img.SetSpacing(original.GetSpacing())\n",
    "    img.SetOrigin(original.GetOrigin())\n",
    "    originalDir = original.GetDirection()\n",
    "    img.SetDirection((originalDir[0], originalDir[1], originalDir[2],\n",
    "                    originalDir[3], originalDir[4], originalDir[5],\n",
    "                    originalDir[6], originalDir[7], originalDir[8]))\n",
    "    pass\n",
    "\n",
    "\n",
    "def generate_dataset_json_method(BASE_PATH, TASK_NAME, VENDOR):\n",
    "    generate_dataset_json(BASE_PATH + TASK_NAME + \"/dataset.json\",\n",
    "        BASE_PATH + TASK_NAME + \"/imagesTr\",\n",
    "        BASE_PATH + TASK_NAME + \"/imagesTs\",\n",
    "        ('T1',),\n",
    "        {0: 'background', 1: 'foreground' },\n",
    "        TASK_NAME,\n",
    "        dataset_description=\"VENDOR \" + VENDOR) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    INPUT_PATH = f\"{os.environ['nnUNet_raw_data_base']}/nnUnet_raw_data_base/prostate_data\"\n",
    "\n",
    "    BASE_PATH = f\"{os.environ['nnUNet_raw_data_base']}/nnUNet_raw_data/\"\n",
    "    TASK_NO = 10\n",
    "    #TASK_NAME = \"Task007_mHeart\" + VENDOR \n",
    "    TASK_LIST = []\n",
    "\n",
    "    for path, dirs, files in os.walk(INPUT_PATH):\n",
    "        for dirs_ in dirs:\n",
    "            TASK_NAME = \"Task0\" + str(TASK_NO) + \"_\" + \"Prostate\" + \"-\"  + dirs_\n",
    "            TASK_LIST.append(TASK_NAME)\n",
    "            \n",
    "            \n",
    "            #create output folder structure\n",
    "            Path(BASE_PATH + TASK_NAME + \"/imagesTr\").mkdir(parents=True, exist_ok=True)\n",
    "            Path(BASE_PATH + TASK_NAME + \"/imagesTs\").mkdir(parents=True, exist_ok=True)\n",
    "            Path(BASE_PATH + TASK_NAME + \"/labelsTr\").mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            TASK_NO = TASK_NO + 1\n",
    "            \n",
    "\n",
    "        for file in files:\n",
    "            dirs_ = os.path.basename(os.path.normpath(pathlib.Path(os.path.join(path, file)).parent.resolve()))\n",
    "\n",
    "            TASK_NAME = next(task for task in TASK_LIST if str(task.rpartition('-')[-1]) == dirs_)\n",
    "            if \"egmentation\" in file: #\"egmentation\" since sometimes it's upper and lowercase\n",
    "                original = sitk.ReadImage(os.path.join(path, file))\n",
    "                arr = sitk.GetArrayFromImage(original).astype(np.float32)\n",
    "                arr[arr != 0] = 1\n",
    "                original = sitk.GetImageFromArray(arr)\n",
    "                copyMetaData(sitk.ReadImage(INPUT_PATH +\"/\"+ dirs_ + \"/\" + file.split(\"_\")[0] + \".nii.gz\"), original)\n",
    "                sitk.WriteImage(original, BASE_PATH + TASK_NAME + \"/labelsTr/\" + dirs_ + \"_\" + str(file.rpartition('_')[0])+ \".nii.gz\")\n",
    "\n",
    "\n",
    "            else: #images\n",
    "                original = sitk.ReadImage(os.path.join(path, file))\n",
    "                sitk.WriteImage(original, BASE_PATH + TASK_NAME + \"/imagesTr/\" + dirs_ + \"_\" + os.path.splitext(os.path.splitext(file)[0])[0] + \"_0000\" + \".nii.gz\")\n",
    "                \n",
    "        \n",
    "    for task in TASK_LIST:\n",
    "        generate_dataset_json_method(BASE_PATH, task, str(task.rpartition('-')[-1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default this script will produce tasks with ids 10 to 15. In case you want different ids assigned to the prostate datasets, you may want to change the ``TASK_NO`` variable, that defines the start id. In the following sections and the othernotebooks, we use the following task ids, names indicating the source dataset:\n",
    "- Task011_Prostate-BIDMC\n",
    "- Task012_Prostate-I2CVB\n",
    "- Task013_Prostate-HK\n",
    "- Task015_Prostate-UCL\n",
    "- Task016_Prostate-RUNMC\n",
    "\n",
    "If your tasks have different names, you can rename the folder names and change the task id. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "After preparing the datasets, we can run the command provided by nnUNet to preprocess the data for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nnUNet_plan_and_preprocess -t 11\n",
    "!nnUNet_plan_and_preprocess -t 12\n",
    "!nnUNet_plan_and_preprocess -t 13\n",
    "!nnUNet_plan_and_preprocess -t 15\n",
    "!nnUNet_plan_and_preprocess -t 16\n",
    "\n",
    "!nnUNet_plan_and_preprocess -t 97\n",
    "!nnUNet_plan_and_preprocess -t 98\n",
    "!nnUNet_plan_and_preprocess -t 99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to generate a train, val, test split. As the nnUNet framework only generates a train, val split, we need to do this on our own by putting some of the training data into an additional test split. Unfortunatly, the nnUNet framework first creates its random split upon starting a training. So for each dataset we need to start a dummy training to force the nnUNet framework to generate the initial train, val split. Feel free to stop the training once you see a progress bar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nnUNet_train_sequential 2d -t 11 -f 0 -num_epoch 1 -save_interval 25 -s seg_outputs --store_csv\n",
    "!nnUNet_train_sequential 2d -t 12 -f 0 -num_epoch 1 -save_interval 25 -s seg_outputs --store_csv\n",
    "!nnUNet_train_sequential 2d -t 13 -f 0 -num_epoch 1 -save_interval 25 -s seg_outputs --store_csv\n",
    "!nnUNet_train_sequential 2d -t 15 -f 0 -num_epoch 1 -save_interval 25 -s seg_outputs --store_csv\n",
    "!nnUNet_train_sequential 2d -t 16 -f 0 -num_epoch 1 -save_interval 25 -s seg_outputs --store_csv\n",
    "\n",
    "!nnUNet_train_sequential 2d -t 97 -f 0 -num_epoch 1 -save_interval 25 -s seg_outputs --store_csv\n",
    "!nnUNet_train_sequential 2d -t 98 -f 0 -num_epoch 1 -save_interval 25 -s seg_outputs --store_csv\n",
    "!nnUNet_train_sequential 2d -t 99 -f 0 -num_epoch 1 -save_interval 25 -s seg_outputs --store_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have generated an initial train, val split, we can run our [create_3_split.py](create_3_split.py) script to add an additional test split by taking 30% of the training cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_3_split.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will copy (symlink) the data and segmentations and add the test split in a new task by incrementing the task ID with 100. Task 11 will become 111, task 12 becomes task 112 and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augemented Datasets\n",
    "As the last step of the preprocessing we need to create copies of our datasets by augmenting them with MRI artifacts to be used in our OoD experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_augmentations import copy_dataset\n",
    "import os\n",
    "import torchio as tio\n",
    "\n",
    "ROOT = os.path.join(os.environ['nnUNet_raw_data_base'], \"nnUnet_raw_data\")\n",
    "\n",
    "transform = tio.OneOf({\n",
    "    tio.RandomSpike(intensity=(3,5)): 1,\n",
    "    tio.RandomBiasField(1): 1,\n",
    "    tio.RandomGhosting(intensity=(3,5)): 1,\n",
    "})\n",
    "\n",
    "for src_task_name, dst_task_name in [(\"Task197_DecathHip\", \"Task400_DecathHipAugmented\"),\n",
    "                                     (\"Task198_Dryad\", \"Task401_DryadAugmented\"),\n",
    "                                     (\"Task199_HarP\", \"Task402_HarPAugmented\"),\n",
    "                                     \n",
    "                                     (\"Task111_Prostate-BIDMC\", \"Task403_Prostate-BIDMCAugmented\"),\n",
    "                                     (\"Task112_Prostate-I2CVB\", \"Task404_Prostate-I2CVBAugmented\"),\n",
    "                                     (\"Task113_Prostate-HK\", \"Task405_Prostate-HKAugmented\"),\n",
    "                                     (\"Task115_Prostate-UCL\", \"Task406_Prostate-UCLAugmented\"),\n",
    "                                     (\"Task116_Prostate-RUNMC\", \"Task407_Prostate-RUNMCAugmented\")]:\n",
    "\n",
    "    copy_dataset(os.path.join(ROOT, src_task_name), os.path.join(ROOT, dst_task_name), transform)\n",
    "    os.makedirs(os.path.join(os.environ[\"nnUNet_preprocessed\"], dst_task_name), exist_ok=True)\n",
    "    if not os.path.exists(os.path.join(os.environ[\"nnUNet_preprocessed\"], dst_task_name, \"splits_final.pkl\")):\n",
    "        os.symlink(os.path.join(os.environ[\"nnUNet_preprocessed\"], src_task_name, \"splits_final.pkl\"), \n",
    "                os.path.join(os.environ[\"nnUNet_preprocessed\"], dst_task_name, \"splits_final.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
