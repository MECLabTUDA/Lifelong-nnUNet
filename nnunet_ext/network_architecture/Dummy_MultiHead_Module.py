#########################################################################################################
#----------This class represents a Generic Module enabling multiple heads for any network.--------------#
#########################################################################################################

import copy
from torch import nn
from typing import Type
from operator import attrgetter

class Dummy_MultiHead_Module(nn.Module):
    r"""This class is a Module that can be used for any task where multiple heads using a shared body
        are necessary. The heads are stored in a ModuleDict, whereas the task name is the key to the
        corresponding head/module. This class can be used for any network, since the network class object
        needs to be provided as well.
    """
    def __init__(self, class_object: Type[nn.Module], split_at, task, prev_trainer=None, *args, **kwargs):
        r"""Constructor of the Module for multiple heads. 
            :param class_object: This is the class (network) that should be used, ie. that will be split. This needs to
                                 be a class inheriting nn.Module, where the .forward() method is implemented, since this
                                 will be used as well.
            :param split_at: The path in the network where the split should be performed. Use the dot notation ('.') to
                             specify the path to a desired split point.
            :param task: The name of the first head. Since this function will perform a split, the splitted part needs to
                         have a name; this specifies it.
            :param prev_trainer: If the split should be performed on a previously trained/existing model, than this can be provided
                                 with this variable. NOTE: The type needs be equal to the provided class_object.
            :param *args, **kwargs: Provide all further positional and keyword arguments that are necessary by the class_object
                                    class when performing an initialization. NOTE: This needs to be done correctly, since
                                    if it is not, class_object has missing/too much positional arguemnts and will fail during
                                    runtime in initialization. This is only necessary when prev_trainer is not provided or None.
            NOTE: The model that can be accessed using self.model represents the running model, and is of the same type as 
                  class_object. 'self' btw is a MultiHead_Module consisting of a body (self.body), heads (self.heads) and
                  the running model (self.model) based on the activated task (self.active_task). When training on a new task
                  is desired, the useer needs to activate this task before calling .forward() in such a way that the running
                  model has the right parameters and structure loaded etc. Tasks are activated using self.assemble_model(..).
                  As mentioned before, prev_trainer is not necessary but can be used when a pre-trained model should be used
                  as initialization and not a complete new initialized class_object. Further, all tasks that will be added
                  using self.add_new_task(..) use the Module that is extracted based on to the split from the prev_trainer model,
                  including its state_dict etc. if a prev_trainer is used, otherwise the head from a new initialized class_object
                  is used.
        """
        # -- Initialize using super -- #
        super().__init__()
        self.class_object = class_object
        self.model = self.class_object(*args, **kwargs)

        # -- Store the class_object -- #
        self.class_object = class_object
        # -- Define empty ModuleDict for all the heads -- #
        self.heads = {}

        # -- Define a variable that specifies the active_task -- #
        assert isinstance(task, (str, int)), "The provided task needs to be an integer (ID) or string, not {}..".format(type(task))
        self.active_task = task
        self.state_init = None
        self.heads[self.active_task] = None
        self.body_freezed = False

    def forward(self, x):
        r"""Forward pass during training --> task needs to be specified before calling forward.
            Assemble the model before callinng this function.
            NOTE: Follow with a split of the model after the backward pass to update the head and the body;
                  use self.update_after_iteration(..) for it.
        """
        # -- Let the class_object do the work since the assembled model is an object of this class -- #
        res = self.class_object.forward(self.model, x) # --> Do not use super, since we want to set the correct self object ;)

        # -- Return the forward result generated by Generic_UNet.forward -- #
        return res

    def update_after_iteration(self, model=None, update_body=True):
        r"""This function is used to update the head and body. This should be used after every backward pass
            of the network that is trained on.
            :param model: The model that should be split and with which the parameters for self.body and the
                          current head are updated.
            :param update_body: This boolean flag identifies if the body should be updated as well or only the
                                head.
        """
        # -- If model is None set it to self.model -- #
        pass

    def assemble_model(self, task, freeze_body=False):
        r"""This function assembles a desired model structure based on self.body and corresponding head based
            on the provided task. The parameters of the self.model are updated using the assembled_model composing
            of self.body and specified head.
            :param task: Task name of the head to specify which head should be joined with the body.
            :param freeze_body: Specify if the body weights should be freezed or not.
            :return: Function returns the running model (self.model)
        """
        return self.model

    
    def add_new_task(self, task, use_init, model=None):
        r"""Use this function to add the initial module from on the first split.
            Specify the task name with which it will be registered in the ModuleDict.
            :param task: Task name of the new task (key for the ModuleDict)
            :param model: nn.Module that represent the new task. If this is None, the new head
                          is a copy from the very first splitted head during initialization
            NOTE: If the task already exists, it will be overwritten. If the user provides
                  a model, than ensure that it works with the forward function from the
                  class_object. If this does not map than an error will be thrown later on.
        """
        # -- Create a new task in self.heads with the module from the first split -- #
        if model is None:
            # -- Add the latest task -- #
            self.heads[task] = None
            
        else:
            # -- Register the provided model -- #
            self.heads[task] = None

    def add_n_tasks_and_activate(self, list_of_tasks, activate_with, remove_old_tasks=True):
        r"""Use this function to initialize for each task name in the list a new head. --> Important when restoring a model,
            since the heads are not created in this init function and need to be set manually before loading a state_dict
            that includes n heads. Further, self.model will be assembled based on activate_with. In the case of calling
            this function before restoring, the user needs to provide the correct activate_with, ie. the head that was activated
            in times of saving the Multi Head Network. otherwise there will be a mixup that might be difficult to spot (wrong head
            with wrong state_dict eg.).
            :param list_of_tasks: List of strings representing the task names
            :param activate_with: String, for instance a task that is used to assemble self.model
            :param remove_old_tasks: Bool, indicating if all heads that are not mentioned in list_of_tasks should be removed
        """
        # -- Loop through list of tasks -- #
        for task in list_of_tasks:
            # -- Add the task to the head if it does not exist-- #
            if task not in self.heads:
                self.add_new_task(task, use_init=True)

        # -- Remove all heads that are not in the list_of_tasks if desired -- #
        if remove_old_tasks:
            for task in list(self.heads.keys()):
                if task not in list_of_tasks:
                    # -- Remove it from the head -- #
                    del self.heads[task]

        # -- Assemble the model based on activate_with -- #
        self.assemble_model(activate_with)
