{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is wrong with Continual Learning in Medical Image Segmentation - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "For basic setup of the Lifelong nnUNet framework, please refer to the [Github repository](https://github.com/MECLabTUDA/Lifelong-nnUNet). Please note that you need so switch to the branch dev-task_agnostic_cl, before running the installation via pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "After setting up the Framework you need to prepare the datasets. Therefore the datasets need to be downloaded and placed in the nnUNet_data_base/nnUNet_raw_data folder. \n",
    "\n",
    "* [Cardiac](https://www.ub.edu/mnms/)\n",
    "* [Prostate](https://liuquande.github.io/SAML/)\n",
    "\n",
    "To prepare the **cardiac** database you need to run the following script. You need to adjust the VENDOR variable (the M&M datasets comes with vendors A and B) and the TASK_NAME variable to the task name that you want to assign. In this tutorial we will use the tasks Task008_mHeartA and Task009_mHeartB for the vendors A and B respectivly. Furthermore the INPUT_PATH of the input dataset and the BASE_PATH of your nnUNet_raw_data folder need to be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from nnunet.dataset_conversion.utils import generate_dataset_json\n",
    "\n",
    "\n",
    "INPUT_PATH = \".../m_m_challenge_dataset/\"\n",
    "VENDOR = \"A\"\n",
    "\n",
    "BASE_PATH = \".../nnUnet_raw_data_base/nnUNet_raw_data\"\n",
    "TASK_NAME = \"Task008_mHeart\" + VENDOR \n",
    "\n",
    "\n",
    "#create output folder structure\n",
    "Path(BASE_PATH + TASK_NAME + \"/imagesTr\").mkdir(parents=True, exist_ok=True)\n",
    "Path(BASE_PATH + TASK_NAME + \"/imagesTs\").mkdir(parents=True, exist_ok=True)\n",
    "Path(BASE_PATH + TASK_NAME + \"/labelsTr\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def copyMetaData(original, img):\n",
    "    img.SetSpacing(original.GetSpacing())\n",
    "    img.SetOrigin(original.GetOrigin())\n",
    "    originalDir = original.GetDirection()\n",
    "    img.SetDirection((originalDir[0], originalDir[1], originalDir[2],\n",
    "                    originalDir[4], originalDir[5], originalDir[6],\n",
    "                    originalDir[8], originalDir[9], originalDir[10]))\n",
    "    pass\n",
    "    \n",
    "\n",
    "def extractTimeSlice(original, timeSlice):\n",
    "    arr = sitk.GetArrayFromImage(original).astype(np.float32)\n",
    "    arr = arr[timeSlice]\n",
    "    img = sitk.GetImageFromArray(arr)\n",
    "    \n",
    "    img.SetSpacing(original.GetSpacing())\n",
    "    img.SetOrigin(original.GetOrigin())\n",
    "    \n",
    "    originalDir = original.GetDirection()\n",
    "    img.SetDirection((originalDir[0], originalDir[1], originalDir[2],\n",
    "                    originalDir[4], originalDir[5], originalDir[6],\n",
    "                    originalDir[8], originalDir[9], originalDir[10]))\n",
    "    return img\n",
    "    \n",
    "\n",
    "def processLabeledImage(inputPath, inName, outName, timeSlice):\n",
    "    original = sitk.ReadImage(inputPath + inName + \"_sa.nii.gz\")\n",
    "    img = extractTimeSlice(original, timeSlice)\n",
    "    sitk.WriteImage(img, BASE_PATH + TASK_NAME + \"/imagesTr/\" + outName + \".nii.gz\")\n",
    "    pass\n",
    "def processUnlabeledImage(inputPath, inName, outName, timeSlice):\n",
    "    original = sitk.ReadImage(inputPath + inName + \"_sa.nii.gz\")\n",
    "    img = extractTimeSlice(original, timeSlice)\n",
    "    sitk.WriteImage(img, BASE_PATH + TASK_NAME + \"/imagesTs/\" + outName + \".nii.gz\")\n",
    "    pass\n",
    "def processSegmentationImage(inputPath, inName, outName, timeSlice):\n",
    "    original = sitk.ReadImage(inputPath + inName + \"_sa_gt.nii.gz\")\n",
    "    arr = sitk.GetArrayFromImage(original).astype(np.float32)\n",
    "    arr = arr[timeSlice]\n",
    "    img = sitk.GetImageFromArray(arr)\n",
    "    \n",
    "    copyMetaData(original, img)\n",
    "    sitk.WriteImage(img, BASE_PATH + TASK_NAME + \"/labelsTr/\" + outName + \".nii.gz\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "csv = pd.read_csv(INPUT_PATH + \"M&Ms Dataset Information.csv\", sep='\\t')\n",
    "labeledData = os.listdir(INPUT_PATH + \"Training-corrected/Labeled\")\n",
    "unlabeledData = os.listdir(INPUT_PATH + \"Training-corrected/Unlabeled\")\n",
    "\n",
    "csv = csv[csv.Vendor == VENDOR]\n",
    "\n",
    "print(csv)\n",
    "index = 0\n",
    "\n",
    "for inName, timeSlice in zip(csv[\"External code\"], csv[\"ED\"]):\n",
    "    outName = inName + \"_0000\" #index for modality: 0\n",
    "    index += 1\n",
    "    if inName in labeledData:\n",
    "        print(inName, \"labeled\")\n",
    "        processLabeledImage(INPUT_PATH + \"Training-corrected/Labeled/\" + inName + \"/\", inName, outName, timeSlice)\n",
    "        processSegmentationImage(INPUT_PATH + \"Training-corrected/Labeled/\" + inName + \"/\", inName, inName, timeSlice)\n",
    "    else:\n",
    "        print(inName, \"unlabeled\")\n",
    "        processUnlabeledImage(INPUT_PATH + \"Training-corrected/Unlabeled/\" + inName + \"/\", inName, outName, timeSlice)\n",
    "\n",
    "\n",
    "print(\"finished\")\n",
    "\n",
    "\n",
    "generate_dataset_json(BASE_PATH + TASK_NAME + \"/dataset.json\",\n",
    "    BASE_PATH + TASK_NAME + \"/imagesTr\",\n",
    "    BASE_PATH + TASK_NAME + \"/imagesTs\",\n",
    "    ('MRI',),\n",
    "    {0: 'background', 1: 'LV', 2: 'MYO', 3: 'RV' },\n",
    "    TASK_NAME,\n",
    "    dataset_description=\"VENDOR \" + VENDOR) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To prepare the prostate databse you need to run the following script. Make sure to adapt the INPUT_PATH and BASE_PATH variable to your setup. By default this script will produce task with ids 10 to 15. In case you want different ids assigned to the prostate datasets, you may want to change the TASK_NO variable, that defines the start id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from nnunet.dataset_conversion.utils import generate_dataset_json\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def copyMetaData(original, img):\n",
    "    img.SetSpacing(original.GetSpacing())\n",
    "    img.SetOrigin(original.GetOrigin())\n",
    "    originalDir = original.GetDirection()\n",
    "    img.SetDirection((originalDir[0], originalDir[1], originalDir[2],\n",
    "                    originalDir[3], originalDir[4], originalDir[5],\n",
    "                    originalDir[6], originalDir[7], originalDir[8]))\n",
    "    pass\n",
    "\n",
    "\n",
    "def processLabeledImage(inputPath, inName, outName):\n",
    "    original = sitk.ReadImage(inputPath + inName + \".nii.gz\")\n",
    "    sitk.WriteImage(img, BASE_PATH + TASK_NAME + \"/imagesTr/\" + outName + \".nii.gz\")\n",
    "    pass\n",
    "def processUnlabeledImage(inputPath, inName, outName):\n",
    "    original = sitk.ReadImage(inputPath + inName + \".nii.gz\")\n",
    "    sitk.WriteImage(img, BASE_PATH + TASK_NAME + \"/imagesTs/\" + outName + \".nii.gz\")\n",
    "    pass\n",
    "def processSegmentationImage(inputPath, inName, outName):\n",
    "    original = sitk.ReadImage(inputPath + inName + \".nii.gz\")\n",
    "    arr = sitk.GetArrayFromImage(original).astype(np.float32)\n",
    "    img = sitk.GetImageFromArray(arr)\n",
    "    \n",
    "    copyMetaData(original, img)\n",
    "    sitk.WriteImage(img, BASE_PATH + TASK_NAME + \"/labelsTr/\" + outName + \".nii.gz\")\n",
    "    pass\n",
    "\n",
    "\n",
    "def generate_dataset_json_method(BASE_PATH, TASK_NAME, VENDOR):\n",
    "    generate_dataset_json(BASE_PATH + TASK_NAME + \"/dataset.json\",\n",
    "        BASE_PATH + TASK_NAME + \"/imagesTr\",\n",
    "        BASE_PATH + TASK_NAME + \"/imagesTs\",\n",
    "        ('T2 MRI',),\n",
    "        {0: 'background', 1: 'foreground' },\n",
    "        TASK_NAME,\n",
    "        dataset_description=\"VENDOR \" + VENDOR) \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    INPUT_PATH = \".../prostate_data\"\n",
    "\n",
    "    BASE_PATH = \".../nnUnet_raw_data_base/nnUNet_raw_data/\"\n",
    "    TASK_NO = 10\n",
    "    TASK_LIST = []\n",
    "\n",
    "    for path, dirs, files in os.walk(INPUT_PATH):\n",
    "        for dirs_ in dirs:\n",
    "            TASK_NAME = \"Task0\" + str(TASK_NO) + \"_\" + \"Prostate\" + \"-\"  + dirs_\n",
    "            TASK_LIST.append(TASK_NAME)\n",
    "            \n",
    "            \n",
    "            #create output folder structure\n",
    "            Path(BASE_PATH + TASK_NAME + \"/imagesTr\").mkdir(parents=True, exist_ok=True)\n",
    "            Path(BASE_PATH + TASK_NAME + \"/imagesTs\").mkdir(parents=True, exist_ok=True)\n",
    "            Path(BASE_PATH + TASK_NAME + \"/labelsTr\").mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            TASK_NO = TASK_NO + 1\n",
    "            \n",
    "\n",
    "        for file in files:\n",
    "            dirs_ = os.path.basename(os.path.normpath(pathlib.Path(os.path.join(path, file)).parent.resolve()))\n",
    "\n",
    "            TASK_NAME = next(task for task in TASK_LIST if str(task.rpartition('-')[-1]) == dirs_)\n",
    "            if \"egmentation\" in file: #\"egmentation\" since sometimes it's upper and lowercase and should be more efficient than to upper or to lower\n",
    "                #labels\n",
    "                original = sitk.ReadImage(os.path.join(path, file))\n",
    "                arr = sitk.GetArrayFromImage(original).astype(np.float32)\n",
    "                #arr[arr != 0] = 1\n",
    "                original = sitk.GetImageFromArray(arr)\n",
    "                copyMetaData(sitk.ReadImage(INPUT_PATH +\"/\"+ dirs_ + \"/\" + file.split(\"_\")[0] + \".nii.gz\"), original)\n",
    "                sitk.WriteImage(original, BASE_PATH + TASK_NAME + \"/labelsTr/\" + dirs_ + \"_\" + str(file.rpartition('_')[0])+ \".nii.gz\")\n",
    "\n",
    "\n",
    "            else: #images\n",
    "                original = sitk.ReadImage(os.path.join(path, file))\n",
    "                sitk.WriteImage(original, BASE_PATH + TASK_NAME + \"/imagesTr/\" + dirs_ + \"_\" + os.path.splitext(os.path.splitext(file)[0])[0] + \"_0000\" + \".nii.gz\")\n",
    "                \n",
    "        \n",
    "    for task in TASK_LIST:\n",
    "        generate_dataset_json_method(BASE_PATH, task, str(task.rpartition('-')[-1]))\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To create the joint dataset, you need to manually copy the data from the nnUnet_raw_data_base/nnUNet_raw_data folder to a common folder. \n",
    "\n",
    "For our cardiac data that means we copy the contents of Task008_mHeartA/imagesTr and Task009_mHeartB/imagesTr to Task031_Cardiac_joined/imagesTr. We proceed similar with the labelsTr folders.\n",
    "After that we create the dataset.json file by running this script. Do not forget to set BASE_PATH accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnunet.dataset_conversion.utils import generate_dataset_json\n",
    "\n",
    "BASE_PATH = BASE_PATH = \".../nnUnet_raw_data_base/nnUNet_raw_data\"\n",
    "\n",
    "generate_dataset_json(BASE_PATH + \"Task031_Cardiac_joined/dataset.json\",\n",
    "    BASE_PATH + \"/Task031_Cardiac_joined/imagesTr\",\n",
    "    BASE_PATH + \"/Task031_Cardiac_joined/imagesTs\",\n",
    "    ('MRI',),\n",
    "    {0: 'background', 1: 'LV', 2: 'MYO', 3: 'RV' },\n",
    "    \"Task031_Cardiac_joined\",\n",
    "    dataset_description=\"Merged Cardiac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that you can run the following basic commands to prepare the data for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nnUNet_plan_and_preprocess -t Task008_mHeartA\n",
    "!nnUNet_plan_and_preprocess -t Task009_mHeartB\n",
    "!nnUNet_plan_and_preprocess -t Task031_Cardiac_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step we will manually create the splits file, such that the joint database contains the same train/val split as the separat datasets. What you need to do is copy the splits_final.pkl files from the separate datasets to a common location, rename them to splits_final0.pkl, splits_final1.pkl and run the script below.\n",
    "It will produce a splits_final.pkl file that needs to be pasted into the nnUnet_preprocessed/Task031_Cardiac_joined folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def load_pickle(file, mode='rb'):\n",
    "    with open(file, mode) as f:\n",
    "        a = pickle.load(f)\n",
    "    return a\n",
    "    \n",
    "def write_pickle(obj, file, mode='wb'):\n",
    "    with open(file, mode) as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "        \n",
    "list_of_splits = [load_pickle(\"splits_final0.pkl\"), load_pickle(\"splits_final1.pkl\")]\n",
    "\n",
    "final_split =[]\n",
    "\n",
    "for fold in range(5):\n",
    "    final_at_fold = OrderedDict([('train', []), ('val',[])])\n",
    "    for split in list_of_splits:\n",
    "        final_at_fold['train'] = numpy.concatenate((final_at_fold['train'],split[fold]['train']))\n",
    "        final_at_fold['val'] = numpy.concatenate((final_at_fold['val'],split[fold]['val']))\n",
    "        \n",
    "    final_split.append(final_at_fold)\n",
    "    \n",
    "    \n",
    "print(final_split)\n",
    "write_pickle(final_split, \"splits_final.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
